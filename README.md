Multi-Class Text Classification Using Word Representations and ML/NN Models

This project presents a comparative study of 22 machine learning and neural network models for multi-class text classification using BoW, TF-IDF, GloVe, and Skip-gram embeddings. The findings highlight that GloVe-based LSTM models achieve the highest performance, demonstrating the effectiveness of context-aware embeddings and sequential architectures.
