# Multi-Class Text Classification Using Word Representations and ML/NN Models

This project presents a comparative study of 22 machine learning and neural network models—including Logistic Regression, Naive Bayes, Random Forest, DNN, RNN, GRU, LSTM, and their bidirectional variants—for multi-class text classification using various word representations such as Bag-of-Words (BoW), TF-IDF, GloVe embeddings, and Skip-gram embeddings. The work highlights key performance differences between traditional ML approaches and advanced neural network architectures.



## Accuracy Comparison

The following figure illustrates the comparative accuracy of all models across different word representations:

![Model Accuracy Comparison](accuracy_comparison.png)



## Features

- Comparative analysis of **22 models** on the same dataset  
- Uses multiple text representation methods: BoW, TF-IDF, GloVe, Skip-gram  
- Includes **traditional ML models** (Logistic Regression, Naive Bayes, Random Forest)  
- Includes **neural network architectures** (DNN, RNN, GRU, LSTM, Bi-directional variants)  
- Highlights **performance differences** between ML and NN approaches  



## Requirements

- Python 3.x  
- Libraries: `scikit-learn`, `tensorflow`, `keras`, `numpy`, `pandas`, `matplotlib`  



## Usage

1. Clone the repository:
   ```bash
   https://github.com/nowshinreza/ML-vs-NN-Text-Classification-Benchmark.git
